{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (10000, 11)\n",
      "DataFrame columns: ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'y']\n",
      "         x1        x2          x3        x4        x5        x6        x7  \\\n",
      "0  1.205492  5.823226   98.837539 -1.075852  0.999205  0.911543  3.623558   \n",
      "1  1.391530  3.611581   98.857197 -5.020318  0.677165  0.999492  3.413112   \n",
      "2  1.692571 -0.887019  100.901276 -0.595548  0.177550 -0.915495  4.320264   \n",
      "3  4.289320  1.416843  100.784735 -2.897154 -0.066972 -0.786173  2.093003   \n",
      "4  0.542420 -1.010095  100.015580 -3.070705  0.088324 -0.242669  0.767942   \n",
      "\n",
      "         x8        x9        x10  y  \n",
      "0 -1.720267 -0.346191 -54.708330 -1  \n",
      "1  4.253865  2.041603 -54.317291  1  \n",
      "2  0.907834  3.126815 -56.397484 -1  \n",
      "3  1.336237  2.183829 -56.197728  1  \n",
      "4 -0.284683 -2.104145 -55.794045  1  \n",
      "\n",
      "Basic statistics:\n",
      "                  x1            x2            x3            x4            x5  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       1.591298      0.515879     99.848936     -1.504139      0.077645   \n",
      "std        1.321185      2.054488      0.710948      1.133605      0.707270   \n",
      "min        0.002443     -7.524934     98.572455     -7.078939     -1.000000   \n",
      "25%        0.524758     -0.883862     99.159314     -2.179713     -0.624710   \n",
      "50%        1.276243      0.492813     99.802830     -1.262985      0.157145   \n",
      "75%        2.352106      1.902003    100.549141     -0.605316      0.777856   \n",
      "max        9.384223      8.302375    101.260768     -0.000003      1.000000   \n",
      "\n",
      "                 x6            x7            x8            x9           x10  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.051823      0.975207      0.635194      0.051926    -55.447678   \n",
      "std        0.704755      2.162237      2.212708      1.769646      0.710082   \n",
      "min       -1.000000     -6.906971     -7.140755     -7.151890    -56.773931   \n",
      "25%       -0.644261     -0.500506     -0.879985     -1.129665    -56.137679   \n",
      "50%        0.101648      1.006918      0.624762      0.024726    -55.397208   \n",
      "75%        0.754770      2.434886      2.098389      1.248961    -54.758351   \n",
      "max        1.000000      8.760306      9.287266      6.211452    -54.208890   \n",
      "\n",
      "                  y  \n",
      "count  10000.000000  \n",
      "mean      -0.001600  \n",
      "std        1.000049  \n",
      "min       -1.000000  \n",
      "25%       -1.000000  \n",
      "50%       -1.000000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\\n\", df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of y: [-1  1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We must check how y is encoded. If it's in {0,1}, we might convert to {-1, +1}.\n",
    "# Let's see the unique values of y:\n",
    "unique_y = df['y'].unique()\n",
    "print(\"Unique values of y:\", unique_y)\n",
    "\n",
    "# Convert y to {-1, +1} if it appears to be in {0,1}.\n",
    "# This is often a choice for Perceptron / SVM convenience.\n",
    "# If the dataset is already in {-1, +1}, you can skip this step.\n",
    "if set(unique_y) == {0,1}:\n",
    "    df['y'] = df['y'].apply(lambda val: 1 if val == 1 else -1)\n",
    "    print(\"Converted y from {0,1} to {-1,+1}\")\n",
    "    \n",
    "# Now let's separate features and labels\n",
    "X = df[[f'x{i}' for i in range(1,11)]].values  # shape = (n_samples, 10)\n",
    "y = df['y'].values                             # shape = (n_samples,)\n",
    "\n",
    "# We will standardize (mean 0, std 1) the features to help training stability.\n",
    "# But to avoid data leakage, we will do this transformation AFTER splitting \n",
    "# into train/val/test. We'll just define a function now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "Train: (7000, 10) (7000,)\n",
      "Validation: (1500, 10) (1500,)\n",
      "Test: (1500, 10) (1500,)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 4\n",
    "# (Train/Validation/Test Split)\n",
    "########################################\n",
    "\n",
    "def train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, shuffle=True, seed=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_end = int(train_ratio * n)\n",
    "    val_end = int((train_ratio + val_ratio) * n)\n",
    "    \n",
    "    X_train, y_train = X[indices[:train_end]], y[indices[:train_end]]\n",
    "    X_val, y_val     = X[indices[train_end:val_end]], y[indices[train_end:val_end]]\n",
    "    X_test, y_test   = X[indices[val_end:]], y[indices[val_end:]]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, 0.7, 0.15, 0.15)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 5\n",
    "# (Helper Functions)\n",
    "########################################\n",
    "\n",
    "def standard_scale_train(X_train):\n",
    "    \"\"\"\n",
    "    Computes mean and std of X_train, returns scaled X_train, \n",
    "    plus the (mean, std) for future use on validation/test sets.\n",
    "    \"\"\"\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0, ddof=1)\n",
    "    std[std == 0] = 1.0  # Avoid division by zero if any feature is constant\n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    return X_train_scaled, mean, std\n",
    "\n",
    "def standard_scale_apply(X, mean, std):\n",
    "    \"\"\"\n",
    "    Applies the saved mean, std to scale a new dataset X.\n",
    "    \"\"\"\n",
    "    return (X - mean) / std\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes 0-1 accuracy, i.e., proportion of matching labels.\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def sign_function(scores):\n",
    "    \"\"\"\n",
    "    Given an array of scores, return an array of +1 or -1 based on the sign.\n",
    "    \"\"\"\n",
    "    return np.where(scores >= 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 6\n",
    "# (Linear Models)\n",
    "########################################\n",
    "\n",
    "# 1) PERCEPTRON\n",
    "def perceptron_train(X, y, epochs=10, eta=1.0):\n",
    "    \"\"\"\n",
    "    Trains a linear Perceptron classifier.\n",
    "    X: shape (n_samples, n_features)\n",
    "    y: shape (n_samples,) in {-1, +1}\n",
    "    epochs: number of passes over the dataset\n",
    "    eta: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        w: weight vector, shape (n_features,)\n",
    "        b: bias term (or we can incorporate into w by adding a constant feature)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Initialize parameters\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(n_samples):\n",
    "            # Perceptron update rule\n",
    "            # If y[i]*(w Â· X[i] + b) <= 0, then update\n",
    "            if y[i] * (np.dot(w, X[i]) + b) <= 0:\n",
    "                w += eta * y[i] * X[i]\n",
    "                b += eta * y[i]\n",
    "    return w, b\n",
    "\n",
    "def perceptron_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Predict with a trained Perceptron classifier.\n",
    "    \"\"\"\n",
    "    scores = X @ w + b\n",
    "    return sign_function(scores)\n",
    "\n",
    "\n",
    "# 2) SVM (Pegasos, Hinge Loss)\n",
    "def pegasos_svm_train(X, y, lam=1e-4, max_iters=1000, batch_size=1):\n",
    "    \"\"\"\n",
    "    Pegasos algorithm for SVM with hinge loss.\n",
    "    Implementation for binary classification y in {-1, +1}.\n",
    "    \n",
    "    X: (n_samples, n_features)\n",
    "    y: (n_samples,) in {-1, +1}\n",
    "    lam: regularization parameter\n",
    "    max_iters: total number of iterations (batches)\n",
    "    batch_size: mini-batch size\n",
    "    \n",
    "    Returns:\n",
    "        w: weight vector\n",
    "        (No explicit bias here for simplicity; \n",
    "         we can either incorporate a bias as an extra dimension, \n",
    "         or treat data as is. For simplicity, let's skip bias.)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    t = 0\n",
    "    \n",
    "    for iteration in range(1, max_iters+1):\n",
    "        t += 1\n",
    "        # pick a mini-batch\n",
    "        batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # compute step size\n",
    "        eta_t = 1.0 / (lam * iteration)\n",
    "        \n",
    "        # gradient step\n",
    "        # w <- (1 - eta_t * lambda) w\n",
    "        w = (1 - eta_t * lam) * w\n",
    "        \n",
    "        # For each sample in the batch, check hinge condition\n",
    "        for i in range(batch_size):\n",
    "            if y_batch[i] * np.dot(w, X_batch[i]) < 1:\n",
    "                w += eta_t * y_batch[i] * X_batch[i]\n",
    "        \n",
    "    return w\n",
    "\n",
    "def pegasos_svm_predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict with Pegasos SVM (linear).\n",
    "    \"\"\"\n",
    "    scores = X @ w\n",
    "    return sign_function(scores)\n",
    "\n",
    "\n",
    "# 3) Regularized Logistic Classification (Pegasos style)\n",
    "def pegasos_logistic_train(X, y, lam=1e-4, max_iters=1000, batch_size=1):\n",
    "    \"\"\"\n",
    "    Pegasos-style algorithm for logistic loss:\n",
    "    Minimize: lam/2 * ||w||^2 + average(log(1 + exp(-y_i * w.x_i)))\n",
    "    We'll do a stochastic gradient approach analogous to Pegasos.\n",
    "    \n",
    "    X: (n_samples, n_features)\n",
    "    y: (n_samples,) in {-1, +1}\n",
    "    lam: regularization parameter\n",
    "    max_iters: total number of iterations (batches)\n",
    "    batch_size: mini-batch size\n",
    "    \n",
    "    Returns:\n",
    "        w: weight vector\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    for t in range(1, max_iters+1):\n",
    "        # sample a mini-batch\n",
    "        batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        eta_t = 1.0 / (lam * t)  # step size\n",
    "        \n",
    "        # gradient of regularization\n",
    "        w = (1 - eta_t * lam) * w\n",
    "        \n",
    "        # logistic loss gradient\n",
    "        for i in range(batch_size):\n",
    "            # gradient of log(1 + exp(-y_i * w.dot(x_i))) w.r.t w\n",
    "            # = - (y_i * x_i) * exp(-y_i*w.dot(x_i)) / (1 + exp(-y_i*w.dot(x_i)))\n",
    "            exponent = - y_batch[i] * np.dot(w, X_batch[i])\n",
    "            grad_factor = - (y_batch[i] * X_batch[i]) * (np.exp(exponent) / (1.0 + np.exp(exponent)))\n",
    "            w -= eta_t * grad_factor\n",
    "    \n",
    "    return w\n",
    "\n",
    "def pegasos_logistic_predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict with logistic classifier using sign of w.x.\n",
    "    \"\"\"\n",
    "    scores = X @ w\n",
    "    return sign_function(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 7\n",
    "# (Polynomial Feature Expansion)\n",
    "########################################\n",
    "\n",
    "def polynomial_feature_expansion_degree2(X):\n",
    "    \"\"\"\n",
    "    Given X of shape (n_samples, n_features),\n",
    "    return a new array with polynomial features of degree 2:\n",
    "      - all original features\n",
    "      - all pairwise products x_i * x_j (for i <= j, i.e. squares and cross terms)\n",
    "      \n",
    "    We can choose i < j if we do not want the squared terms repeated. \n",
    "    But let's do i <= j so we explicitly include squares as well:\n",
    "       x1^2, x2^2, ...\n",
    "    The resulting dimension = n_features + n_features*(n_features+1)/2\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # number of new features\n",
    "    # original: n_features\n",
    "    # pairs i <= j: n_features*(n_features+1)//2\n",
    "    # total = n_features + n_features*(n_features+1)//2\n",
    "    poly_size = n_features + (n_features*(n_features+1))//2\n",
    "    \n",
    "    X_poly = np.zeros((n_samples, poly_size))\n",
    "    \n",
    "    # fill in the first n_features\n",
    "    X_poly[:, :n_features] = X\n",
    "    \n",
    "    # now fill in the cross terms\n",
    "    idx = n_features\n",
    "    for i in range(n_features):\n",
    "        for j in range(i, n_features):\n",
    "            X_poly[:, idx] = X[:, i] * X[:, j]\n",
    "            idx += 1\n",
    "    \n",
    "    return X_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 8\n",
    "# (Kernel Methods)\n",
    "########################################\n",
    "\n",
    "# We define two kernels: Gaussian (RBF) and Polynomial.\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (Gaussian) kernel:\n",
    "      k(x,z) = exp(-gamma * ||x - z||^2)\n",
    "    X1: shape (n1, d)\n",
    "    X2: shape (n2, d)\n",
    "    gamma: float\n",
    "    \n",
    "    returns K: shape (n1, n2)\n",
    "    \"\"\"\n",
    "    # We can compute pairwise squared distance in a vectorized manner:\n",
    "    # ||x - z||^2 = ||x||^2 + ||z||^2 - 2 x.z\n",
    "    X1_sq = np.sum(X1**2, axis=1).reshape(-1, 1)  # shape (n1, 1)\n",
    "    X2_sq = np.sum(X2**2, axis=1).reshape(1, -1)  # shape (1, n2)\n",
    "    dist_sq = X1_sq + X2_sq - 2 * np.dot(X1, X2.T)  # shape (n1, n2)\n",
    "    \n",
    "    return np.exp(-gamma * dist_sq)\n",
    "\n",
    "def polynomial_kernel(X1, X2, degree=2, c=1.0):\n",
    "    \"\"\"\n",
    "    Polynomial kernel:\n",
    "      k(x,z) = (x.z + c)^degree\n",
    "    X1: shape (n1, d)\n",
    "    X2: shape (n2, d)\n",
    "    degree: int\n",
    "    c: float\n",
    "    \n",
    "    returns K: shape (n1, n2)\n",
    "    \"\"\"\n",
    "    return (X1 @ X2.T + c) ** degree\n",
    "\n",
    "\n",
    "# 1) Kernelized Perceptron\n",
    "def kernel_perceptron_train(X, y, kernel_func, epochs=10, **kernel_params):\n",
    "    \"\"\"\n",
    "    Kernelized Perceptron.\n",
    "    X: shape (n_samples, n_features)\n",
    "    y: shape (n_samples,) in {-1, +1}\n",
    "    kernel_func: function that takes (X, X, **kernel_params) -> K\n",
    "    epochs: number of epochs\n",
    "    **kernel_params: parameters for the kernel\n",
    "    \n",
    "    Returns:\n",
    "        alpha: shape (n_samples,), the dual coefficients\n",
    "        X (training data) is also stored so we can compute kernel with new points\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    # Initialize alpha\n",
    "    alpha = np.zeros(n_samples)\n",
    "    \n",
    "    # Precompute the kernel matrix on the training set\n",
    "    K = kernel_func(X, X, **kernel_params)  # shape (n_samples, n_samples)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(n_samples):\n",
    "            # prediction = sign( sum_j alpha_j * y_j * K[i,j] )\n",
    "            pred = np.sign(np.sum(alpha * y * K[:, i]))\n",
    "            if pred == 0: \n",
    "                pred = 1  # tie-break\n",
    "            if pred != y[i]:\n",
    "                alpha[i] += 1\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def kernel_perceptron_predict(X_train, y_train, alpha, kernel_func, X_test, **kernel_params):\n",
    "    \"\"\"\n",
    "    Predict function for kernelized perceptron.\n",
    "    alpha, X_train, y_train define the classifier.\n",
    "    We compute: sign( sum_i alpha_i * y_i * k(x_i, x_test) ).\n",
    "    \"\"\"\n",
    "    K_test = kernel_func(X_train, X_test, **kernel_params)  # shape (n_train, n_test)\n",
    "    # shape alpha, y_train => (n_train,)\n",
    "    # We want sum_i alpha_i * y_i * K_test[i, j]\n",
    "    # final shape => (n_test,)\n",
    "    decision = np.dot((alpha * y_train), K_test)\n",
    "    return sign_function(decision)\n",
    "\n",
    "\n",
    "# 2) Kernelized Pegasos for SVM\n",
    "def kernel_pegasos_svm_train(X, y, kernel_func, lam=1e-4, max_iters=1000, **kernel_params):\n",
    "    \"\"\"\n",
    "    Kernelized Pegasos algorithm for SVM.\n",
    "    We store alpha for each training sample.\n",
    "    Pseudocode reference from \"Pegasos: Primal Estimated sub-GrAdient SOlver for SVM\" \n",
    "    (the kernelized version).\n",
    "    \n",
    "    There's a known small typo in some references:\n",
    "    - Make sure the update uses the correct indices and normalizations.\n",
    "    - We'll implement the corrected version here.\n",
    "    \n",
    "    Returns:\n",
    "        alpha: shape (n_samples,)\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    alpha = np.zeros(n_samples)\n",
    "    \n",
    "    # Precompute the kernel matrix\n",
    "    K = kernel_func(X, X, **kernel_params)  # shape (n_samples, n_samples)\n",
    "    \n",
    "    for t in range(1, max_iters+1):\n",
    "        # sample one index i\n",
    "        i = np.random.randint(0, n_samples)\n",
    "        eta_t = 1.0 / (lam * t)\n",
    "        \n",
    "        # decision value: sum_j alpha_j * y_j * K[j, i]\n",
    "        decision_i = np.sum(alpha * y * K[:, i])\n",
    "        \n",
    "        # check margin\n",
    "        if y[i] * decision_i < 1:\n",
    "            alpha[i] = alpha[i] + 1.0 * eta_t\n",
    "        \n",
    "        # projection step: \n",
    "        # We must ensure sum_j alpha_j y_j K[j,k] <= 1 / lambda for each k \n",
    "        # in the dual form. But a simpler approach is to scale alpha if needed\n",
    "        # so that the objective doesn't exceed constraints. We'll do a \n",
    "        # typical Pegasos projection: \n",
    "        norm_factor = np.sqrt(lam) * np.sqrt(np.sum((alpha * y)[:,None] * (alpha * y)[None,:] * K))\n",
    "        if norm_factor > 1.0:\n",
    "            alpha = alpha / norm_factor\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def kernel_pegasos_svm_predict(X_train, y_train, alpha, kernel_func, X_test, **kernel_params):\n",
    "    \"\"\"\n",
    "    Predict function for kernelized Pegasos SVM.\n",
    "    alpha, X_train, y_train define the classifier.\n",
    "    We compute sign( sum_i alpha_i * y_i * k(x_i, x_test) ).\n",
    "    \"\"\"\n",
    "    K_test = kernel_func(X_train, X_test, **kernel_params)  # shape (n_train, n_test)\n",
    "    decision = np.dot((alpha * y_train), K_test)\n",
    "    return sign_function(decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 9\n",
    "# (Hyperparameter Tuning - Example)\n",
    "########################################\n",
    "\n",
    "# For demonstration, let's do a small hyperparameter search for each model.\n",
    "# In practice, you might do a more thorough grid search or cross-validation.\n",
    "\n",
    "# 1) Prepare scaled data for linear models\n",
    "X_train_scaled, mean_train, std_train = standard_scale_train(X_train)\n",
    "X_val_scaled = standard_scale_apply(X_val, mean_train, std_train)\n",
    "X_test_scaled = standard_scale_apply(X_test, mean_train, std_train)\n",
    "\n",
    "# 2) (Optional) Prepare polynomial expanded data\n",
    "X_train_poly = polynomial_feature_expansion_degree2(X_train_scaled)\n",
    "X_val_poly = polynomial_feature_expansion_degree2(X_val_scaled)\n",
    "X_test_poly = polynomial_feature_expansion_degree2(X_test_scaled)\n",
    "\n",
    "# We define a small function to evaluate a set of hyperparameters on the val set \n",
    "# and pick the best. We'll do this for SVM and logistic as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linear SVM (Pegasos) on val set => lam=1e-05, acc=0.6886666666666666\n",
      "Best Polynomial-Expanded SVM (Pegasos) on val set => lam=1e-05, acc=0.9253333333333333\n"
     ]
    }
   ],
   "source": [
    "def tune_linear_svm_pegasos(X_tr, y_tr, X_va, y_va, lam_values=[1e-4, 1e-3, 1e-2], \n",
    "                            max_iters=1000, batch_size=1):\n",
    "    best_acc = -1\n",
    "    best_lam = None\n",
    "    best_w = None\n",
    "    for lam in lam_values:\n",
    "        w = pegasos_svm_train(X_tr, y_tr, lam=lam, max_iters=max_iters, batch_size=batch_size)\n",
    "        y_pred_val = pegasos_svm_predict(X_va, w)\n",
    "        acc = accuracy_score(y_va, y_pred_val)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_lam = lam\n",
    "            best_w = w\n",
    "    return best_w, best_lam, best_acc\n",
    "\n",
    "def tune_logistic_pegasos(X_tr, y_tr, X_va, y_va, lam_values=[1e-4, 1e-3, 1e-2], \n",
    "                            max_iters=1000, batch_size=1):\n",
    "    best_acc = -1\n",
    "    best_lam = None\n",
    "    best_w = None\n",
    "    for lam in lam_values:\n",
    "        w = pegasos_logistic_train(X_tr, y_tr, lam=lam, max_iters=max_iters, batch_size=batch_size)\n",
    "        y_pred_val = pegasos_logistic_predict(X_va, w)\n",
    "        acc = accuracy_score(y_va, y_pred_val)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_lam = lam\n",
    "            best_w = w\n",
    "    return best_w, best_lam, best_acc\n",
    "\n",
    "# Example usage with the raw scaled features (linear) or polynomial expanded\n",
    "w_svm_best, lam_svm_best, acc_svm_best = tune_linear_svm_pegasos(X_train_scaled, y_train, \n",
    "                                                                 X_val_scaled, y_val,\n",
    "                                                                 lam_values=[1e-5, 1e-4, 1e-3],\n",
    "                                                                 max_iters=5000, \n",
    "                                                                 batch_size=10)\n",
    "print(\"Best Linear SVM (Pegasos) on val set => lam={}, acc={}\".format(lam_svm_best, acc_svm_best))\n",
    "\n",
    "w_svm_best_poly, lam_svm_best_poly, acc_svm_best_poly = tune_linear_svm_pegasos(X_train_poly, y_train, \n",
    "                                                                                X_val_poly, y_val,\n",
    "                                                                                lam_values=[1e-5, 1e-4, 1e-3],\n",
    "                                                                                max_iters=5000, \n",
    "                                                                                batch_size=10)\n",
    "print(\"Best Polynomial-Expanded SVM (Pegasos) on val set => lam={}, acc={}\".format(lam_svm_best_poly, acc_svm_best_poly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (Pegasos) Test Accuracy (with best lam=1e-05): 0.6847\n",
      "Polynomial-Expanded SVM (Pegasos) Test Accuracy (with best lam=1e-05): 0.9193\n",
      "\n",
      "Kernel Perceptron (RBF) with gamma=0.1 val accuracy: 0.9260\n",
      "Kernel Pegasos (RBF) with gamma=0.1 val accuracy: 0.8373\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# JUPYTER NOTEBOOK CELL 10\n",
    "# (Model Evaluation and Comparison)\n",
    "########################################\n",
    "\n",
    "# Let's do a final training with the best hyperparams we found for SVM (as an example).\n",
    "# Then evaluate on test set.\n",
    "\n",
    "# We'll demonstrate for the linear SVM with scaled features:\n",
    "y_pred_test = pegasos_svm_predict(X_test_scaled, w_svm_best)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Linear SVM (Pegasos) Test Accuracy (with best lam={}): {:.4f}\".format(lam_svm_best, acc_test))\n",
    "\n",
    "# And for the polynomial expansion version:\n",
    "y_pred_test_poly = pegasos_svm_predict(X_test_poly, w_svm_best_poly)\n",
    "acc_test_poly = accuracy_score(y_test, y_pred_test_poly)\n",
    "print(\"Polynomial-Expanded SVM (Pegasos) Test Accuracy (with best lam={}): {:.4f}\".format(lam_svm_best_poly, acc_test_poly))\n",
    "\n",
    "\n",
    "# Similarly, one would do for logistic or kernel-based methods after choosing best hyperparams.\n",
    "#\n",
    "# For instance, let's do a quick demonstration of kernel methods (without thorough tuning here):\n",
    "# Kernel Perceptron with RBF kernel\n",
    "gamma_test = 0.1\n",
    "epochs_test = 5\n",
    "alpha_kp = kernel_perceptron_train(X_train_scaled, y_train, rbf_kernel, epochs=epochs_test, gamma=gamma_test)\n",
    "y_val_kp = kernel_perceptron_predict(X_train_scaled, y_train, alpha_kp, rbf_kernel, X_val_scaled, gamma=gamma_test)\n",
    "acc_val_kp = accuracy_score(y_val, y_val_kp)\n",
    "print(\"\\nKernel Perceptron (RBF) with gamma={} val accuracy: {:.4f}\".format(gamma_test, acc_val_kp))\n",
    "\n",
    "# Kernel Pegasos SVM with RBF kernel\n",
    "alpha_k_peg = kernel_pegasos_svm_train(X_train_scaled, y_train, rbf_kernel, lam=1e-4, max_iters=1000, gamma=gamma_test)\n",
    "y_val_k_peg = kernel_pegasos_svm_predict(X_train_scaled, y_train, alpha_k_peg, rbf_kernel, X_val_scaled, gamma=gamma_test)\n",
    "acc_val_k_peg = accuracy_score(y_val, y_val_k_peg)\n",
    "print(\"Kernel Pegasos (RBF) with gamma={} val accuracy: {:.4f}\".format(gamma_test, acc_val_k_peg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "klc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
